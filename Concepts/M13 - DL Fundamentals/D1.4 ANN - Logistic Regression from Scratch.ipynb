{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential Libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import random\n",
    "from typing import Callable, Dict, Tuple, List\n",
    "\n",
    "# ML\n",
    "from sklearn import datasets\n",
    "# import tensorflow as tf\n",
    "# import keras\n",
    "\n",
    "# Plots\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', None);\n",
    "pd.options.display.max_seq_items = 8000;\n",
    "pd.options.display.max_rows = 8000;\n",
    "%precision 6\n",
    "\n",
    "# Ignore Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "# Define data and image directory\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT_DIR, \"data\")\n",
    "IMAGE_DIR = os.path.join(PROJECT_ROOT_DIR, \"img\")\n",
    "\n",
    "from IPython.display import Image\n",
    "for d in [DATA_DIR, IMAGE_DIR]:\n",
    "    if not os.path.exists(d):\n",
    "        os.mkdir(d)\n",
    "\n",
    "# Define a function to save images\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGE_DIR, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "    \n",
    "from pprint import pprint\n",
    "\n",
    "# This is a little recursive helper function converts all nested ndarrays to python list of lists so that pretty printer knows what to do.\n",
    "\n",
    "def arrayToList(arr):\n",
    "    if type(arr) == type(np.array):\n",
    "        #If the passed type is an ndarray then convert it to a list and\n",
    "        #recursively convert all nested types\n",
    "        return arrayToList(arr.tolist())\n",
    "    else:\n",
    "        #if item isn't an ndarray leave it as is.\n",
    "        return arr\n",
    "    \n",
    "# Suppress exponential notation, define an appropriate float formatter\n",
    "# Specify stdout line width and let pretty print do the work\n",
    "np.set_printoptions(suppress=True, formatter={'float_kind':'{:12.4f}'.format}, linewidth=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import ndarray\n",
    "from typing import List\n",
    "\n",
    "def assert_same_shape(array: ndarray,\n",
    "                      array_grad: ndarray):\n",
    "    assert array.shape == array_grad.shape, \\\n",
    "        '''\n",
    "        Two ndarrays should have the same shape;\n",
    "        instead, first ndarray's shape is {0}\n",
    "        and second ndarray's shape is {1}.\n",
    "        '''.format(tuple(array_grad.shape), tuple(array.shape))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operation(object):\n",
    "    '''\n",
    "    Base class for an \"operation\" in a neural network.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, input_: ndarray):\n",
    "        '''\n",
    "        Stores input in the self._input instance variable\n",
    "        Calls the self._output() function.\n",
    "        '''\n",
    "        self.input_ = input_\n",
    "\n",
    "        self.output = self._output()\n",
    "\n",
    "        return self.output\n",
    "\n",
    "\n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Calls the self._input_grad() function.\n",
    "        Checks that the appropriate shapes match.\n",
    "        '''\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "\n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "\n",
    "        assert_same_shape(self.input_, self.input_grad)\n",
    "        return self.input_grad\n",
    "\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        '''\n",
    "        The _output method must be defined for each Operation\n",
    "        '''\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        The _input_grad method must be defined for each Operation\n",
    "        '''\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParamOperation(Operation):\n",
    "    '''\n",
    "    An Operation with parameters.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, param: ndarray) -> ndarray:\n",
    "        '''\n",
    "        The ParamOperation method\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.param = param\n",
    "\n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Calls self._input_grad and self._param_grad.\n",
    "        Checks appropriate shapes.\n",
    "        '''\n",
    "\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "\n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "        self.param_grad = self._param_grad(output_grad)\n",
    "\n",
    "        assert_same_shape(self.input_, self.input_grad)\n",
    "        assert_same_shape(self.param, self.param_grad)\n",
    "\n",
    "        return self.input_grad\n",
    "\n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Every subclass of ParamOperation must implement _param_grad.\n",
    "        '''\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(a: np.ndarray):\n",
    "    other = 1 - a\n",
    "    return np.concatenate([a, other], axis=1)\n",
    "\n",
    "def unnormalize(a: np.ndarray):\n",
    "    return a[np.newaxis, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import logsumexp\n",
    "\n",
    "def softmax(x, axis=None):\n",
    "    return np.exp(x - logsumexp(x, axis=axis, keepdims=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific `Operation`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightMultiply(ParamOperation):\n",
    "    '''\n",
    "    Weight multiplication operation for a neural network.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, W: ndarray):\n",
    "        '''\n",
    "        Initialize Operation with self.param = W.\n",
    "        '''\n",
    "        super().__init__(W)\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        '''\n",
    "        Compute output.\n",
    "        '''\n",
    "        return np.dot(self.input_, self.param)\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Compute input gradient.\n",
    "        '''\n",
    "        return np.dot(output_grad, np.transpose(self.param, (1, 0)))\n",
    "\n",
    "    def _param_grad(self, output_grad: ndarray)  -> ndarray:\n",
    "        '''\n",
    "        Compute parameter gradient.\n",
    "        '''        \n",
    "        return np.dot(np.transpose(self.input_, (1, 0)), output_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasAdd(ParamOperation):\n",
    "    '''\n",
    "    Compute bias addition.\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 B: ndarray):\n",
    "        '''\n",
    "        Initialize Operation with self.param = B.\n",
    "        Check appropriate shape.\n",
    "        '''\n",
    "        assert B.shape[0] == 1\n",
    "        \n",
    "        super().__init__(B)\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        '''\n",
    "        Compute output.\n",
    "        '''\n",
    "        return self.input_ + self.param\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Compute input gradient.\n",
    "        '''\n",
    "        return np.ones_like(self.input_) * output_grad\n",
    "\n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Compute parameter gradient.\n",
    "        '''\n",
    "        param_grad = np.ones_like(self.param) * output_grad\n",
    "        return np.sum(param_grad, axis=0).reshape(1, param_grad.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Operation):\n",
    "    '''\n",
    "    Sigmoid activation function.\n",
    "    '''\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        '''Pass'''\n",
    "        super().__init__()\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        '''\n",
    "        Compute output.\n",
    "        '''\n",
    "        return 1.0/(1.0+np.exp(-1.0 * self.input_))\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Compute input gradient.\n",
    "        '''\n",
    "        sigmoid_backward = self.output * (1.0 - self.output)\n",
    "        input_grad = sigmoid_backward * output_grad\n",
    "        return input_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Operation):\n",
    "    '''\n",
    "    \"Identity\" activation function\n",
    "    '''\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        '''Pass'''        \n",
    "        super().__init__()\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        '''Pass through'''\n",
    "        return self.input_\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''Pass through'''\n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and MeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "    '''\n",
    "    The \"loss\" of a neural network\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        '''Pass'''\n",
    "        pass\n",
    "\n",
    "    def forward(self, prediction: ndarray, target: ndarray) -> float:\n",
    "        '''\n",
    "        Computes the actual loss value\n",
    "        '''\n",
    "        assert_same_shape(prediction, target)\n",
    "\n",
    "        self.prediction = prediction\n",
    "        self.target = target\n",
    "\n",
    "        loss_value = self._output()\n",
    "\n",
    "        return loss_value\n",
    "\n",
    "    def backward(self) -> ndarray:\n",
    "        '''\n",
    "        Computes gradient of the loss value with respect to the input to the loss function\n",
    "        '''\n",
    "        self.input_grad = self._input_grad()\n",
    "\n",
    "        assert_same_shape(self.prediction, self.input_grad)\n",
    "\n",
    "        return self.input_grad\n",
    "\n",
    "    def _output(self) -> float:\n",
    "        '''\n",
    "        Every subclass of \"Loss\" must implement the _output function.\n",
    "        '''\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _input_grad(self) -> ndarray:\n",
    "        '''\n",
    "        Every subclass of \"Loss\" must implement the _input_grad function.\n",
    "        '''\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class MeanSquaredError(Loss):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        '''Pass'''\n",
    "        super().__init__()\n",
    "\n",
    "    def _output(self) -> float:\n",
    "        '''\n",
    "        Computes the per-observation squared error loss\n",
    "        '''\n",
    "        loss = (\n",
    "            np.sum(np.power(self.prediction - self.target, 2)) / \n",
    "            self.prediction.shape[0]\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _input_grad(self) -> ndarray:\n",
    "        '''\n",
    "        Computes the loss gradient with respect to the input for MSE loss\n",
    "        '''        \n",
    "\n",
    "        return 2.0 * (self.prediction - self.target) / self.prediction.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxCrossEntropy(Loss):\n",
    "    def __init__(self, eps: float=1e-9) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.single_class = False\n",
    "\n",
    "    def _output(self) -> float:\n",
    "\n",
    "        # if the network is just outputting probabilities\n",
    "        # of just belonging to one class:\n",
    "        if self.target.shape[1] == 0:\n",
    "            self.single_class = True\n",
    "\n",
    "        # if \"single_class\", apply the \"normalize\" operation defined above:\n",
    "        if self.single_class:\n",
    "            self.prediction, self.target = \\\n",
    "            normalize(self.prediction), normalize(self.target)\n",
    "\n",
    "        # applying the softmax function to each row (observation)\n",
    "        softmax_preds = softmax(self.prediction, axis=1)\n",
    "\n",
    "        # clipping the softmax output to prevent numeric instability\n",
    "        self.softmax_preds = np.clip(softmax_preds, self.eps, 1 - self.eps)\n",
    "\n",
    "        # actual loss computation\n",
    "        softmax_cross_entropy_loss = (\n",
    "            -1.0 * self.target * np.log(self.softmax_preds) - \\\n",
    "                (1.0 - self.target) * np.log(1 - self.softmax_preds)\n",
    "        )\n",
    "\n",
    "        return np.sum(softmax_cross_entropy_loss) / self.prediction.shape[0]\n",
    "\n",
    "    def _input_grad(self) -> ndarray:\n",
    "\n",
    "        # if \"single_class\", \"un-normalize\" probabilities before returning gradient:\n",
    "        if self.single_class:\n",
    "            return unnormalize(self.softmax_preds - self.target)\n",
    "        else:\n",
    "            return (self.softmax_preds - self.target) / self.prediction.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Regressor object: Our initial 'Layer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 loss: Loss,\n",
    "                 activation: Operation = Sigmoid(),\n",
    "                 seed: int =1):\n",
    "        '''\n",
    "        The number of \"neurons\" roughly corresponds to the \"breadth\" of the layer\n",
    "        '''\n",
    "        self.first = True\n",
    "        self.activation = activation\n",
    "        self.loss = loss\n",
    "        self.params: List[ndarray] = []\n",
    "        self.param_grads: List[ndarray] = []\n",
    "        self.operations: List[Operation] = []\n",
    "        self.seed = seed\n",
    "        if seed:\n",
    "            setattr(self, \"seed\", self.seed)\n",
    "\n",
    "    def _setup_layer(self, input_: ndarray) -> None:\n",
    "        '''\n",
    "        The _setup_layer function must be implemented for each layer\n",
    "        '''\n",
    "        \n",
    "        if self.seed:\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "        self.params = []\n",
    "\n",
    "        # weights\n",
    "        self.params.append(np.random.randn(input_.shape[1], 1)) # Weight initialization\n",
    "\n",
    "        # bias\n",
    "        self.params.append(np.random.randn(1, 1))\n",
    "\n",
    "        self.operations = [WeightMultiply(self.params[0]),\n",
    "                           BiasAdd(self.params[1]),\n",
    "                           self.activation]\n",
    "\n",
    "        return None\n",
    "\n",
    "    def forward(self, input_: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Passes input forward through a series of operations\n",
    "        ''' \n",
    "        if self.first:\n",
    "            self._setup_layer(input_)\n",
    "            self.first = False\n",
    "\n",
    "        self.input_ = input_\n",
    "\n",
    "        for operation in self.operations:\n",
    "\n",
    "            input_ = operation.forward(input_)\n",
    "\n",
    "        self.output = input_\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Passes output_grad backward through a series of operations\n",
    "        Checks appropriate shapes\n",
    "        '''\n",
    "\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "\n",
    "        for operation in reversed(self.operations):\n",
    "            output_grad = operation.backward(output_grad)\n",
    "\n",
    "        input_grad = output_grad\n",
    "        \n",
    "        self._param_grads()\n",
    "\n",
    "        return input_grad\n",
    "\n",
    "    def _param_grads(self) -> ndarray:\n",
    "        '''\n",
    "        Extracts the _param_grads from a layer's operations\n",
    "        '''\n",
    "\n",
    "        self.param_grads = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__, ParamOperation):\n",
    "                self.param_grads.append(operation.param_grad)\n",
    "\n",
    "    def _params(self) -> ndarray:\n",
    "        '''\n",
    "        Extracts the _params from a layer's operations\n",
    "        '''\n",
    "\n",
    "        self.params = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__, ParamOperation):\n",
    "                self.params.append(operation.param)\n",
    "                \n",
    "    def train_batch(self,\n",
    "                    x_batch: ndarray,\n",
    "                    y_batch: ndarray) -> float:\n",
    "        '''\n",
    "        Passes data forward through the layers.\n",
    "        Computes the loss.\n",
    "        Passes data backward through the layers.\n",
    "        '''\n",
    "        \n",
    "        predictions = self.forward(x_batch)\n",
    "\n",
    "        loss = self.loss.forward(predictions, y_batch)\n",
    "\n",
    "        self.backward(self.loss.backward())\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def params(self):\n",
    "        '''\n",
    "        Gets the parameters for the Regressor.\n",
    "        '''\n",
    "        yield from self.params\n",
    "\n",
    "    def param_grads(self):\n",
    "        '''\n",
    "        Gets the gradient of the loss with respect to the parameters for the Regressor.\n",
    "        '''\n",
    "        yield from self.param_grads    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Optimizer` and `SGD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    '''\n",
    "    Base class for a neural network optimizer.\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 lr: float = 0.01):\n",
    "        '''\n",
    "        Every optimizer must have an initial learning rate.\n",
    "        '''\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self) -> None:\n",
    "        '''\n",
    "        Every optimizer must implement the \"step\" function.\n",
    "        '''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    '''\n",
    "    Stochasitc gradient descent optimizer.\n",
    "    '''    \n",
    "    def __init__(self,\n",
    "                 lr: float = 0.01) -> None:\n",
    "        '''Pass'''\n",
    "        super().__init__(lr)\n",
    "\n",
    "    def step(self):\n",
    "        '''\n",
    "        For each parameter, adjust in the appropriate direction, with the magnitude of the adjustment \n",
    "        based on the learning rate.\n",
    "        '''\n",
    "        for (param, param_grad) in zip(self.model.params,\n",
    "                                       self.model.param_grads):\n",
    "\n",
    "            param -= self.lr * param_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Model with Regressor and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from typing import Tuple\n",
    "\n",
    "class LogisticRegressor(object):\n",
    "    '''\n",
    "    Trains a neural network\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 model: Regressor,\n",
    "                 optim: Optimizer) -> None:\n",
    "        '''\n",
    "        Requires a neural network and an optimizer in order for training to occur. \n",
    "        Assign the neural network as an instance variable to the optimizer.\n",
    "        '''\n",
    "        \n",
    "        self.model = model\n",
    "        self.optim = optim\n",
    "        self.best_loss = 1e10\n",
    "        setattr(self.optim, 'model', self.model)\n",
    "        self.train_losses = []\n",
    "        self.validation_losses = []\n",
    "        \n",
    "        \n",
    "    def generate_batches(self,\n",
    "                         X: ndarray,\n",
    "                         y: ndarray,\n",
    "                         size: int = 16) -> Tuple[ndarray]:\n",
    "        '''\n",
    "        Generates batches for training \n",
    "        '''\n",
    "        assert X.shape[0] == y.shape[0], \\\n",
    "        '''\n",
    "        features and target must have the same number of rows, instead\n",
    "        features has {0} and target has {1}\n",
    "        '''.format(X.shape[0], y.shape[0])\n",
    "\n",
    "        N = X.shape[0]\n",
    "\n",
    "        for ii in range(0, N, size):\n",
    "            X_batch, y_batch = X[ii:ii+size], y[ii:ii+size]\n",
    "\n",
    "            yield X_batch, y_batch\n",
    "\n",
    "            \n",
    "    def fit(self, X_train: ndarray, y_train: ndarray,\n",
    "            X_test: ndarray, y_test: ndarray,\n",
    "            epochs: int=100,\n",
    "            eval_every: int=10,\n",
    "            batch_size: int=32,\n",
    "            seed: int = 1,\n",
    "            restart: bool = True)-> None:\n",
    "        '''\n",
    "        Fits the neural network on the training data for a certain number of epochs.\n",
    "        Every \"eval_every\" epochs, it evaluated the neural network on the testing data.\n",
    "        '''\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        if restart:\n",
    "            self.model.first = True\n",
    "\n",
    "            self.best_loss = 1e10\n",
    "\n",
    "        for e in range(epochs):\n",
    "\n",
    "            if (e+1) % eval_every == 0:\n",
    "                \n",
    "                # for early stopping\n",
    "                last_model = deepcopy(self.model)\n",
    "\n",
    "#             X_train, y_train = permute_data(X_train, y_train)\n",
    "\n",
    "            batch_generator = self.generate_batches(X_train, y_train,\n",
    "                                                    batch_size)\n",
    "\n",
    "            for ii, (X_batch, y_batch) in enumerate(batch_generator):\n",
    "\n",
    "                self.model.train_batch(X_batch, y_batch)\n",
    "\n",
    "                self.optim.step()\n",
    "                \n",
    "                self.train_losses.append(self.model.train_batch(X_batch, y_batch))\n",
    "                \n",
    "                \n",
    "            if (e+1) % eval_every == 0:\n",
    "\n",
    "                test_preds = self.model.forward(X_test)\n",
    "                loss = self.model.loss.forward(test_preds, y_test)\n",
    "                self.validation_losses.append(loss)\n",
    "\n",
    "                if loss < self.best_loss:\n",
    "                    print(f\"Validation loss after {e+1} epochs is {loss:.3f}\")\n",
    "                    self.best_loss = loss\n",
    "                    self.validation_losses.append(self.best_loss)\n",
    "                    \n",
    "                else:\n",
    "                    print(f\"\"\"Loss increased after epoch {e+1}, \n",
    "                            final loss was {self.best_loss:.3f}, using the model from epoch {e+1-eval_every}\"\"\")\n",
    "                    self.model = last_model\n",
    "                    # ensure self.optim is still updating self.net\n",
    "                    setattr(self.optim, 'net', self.model)\n",
    "                    break\n",
    "                    \n",
    "    def predict_proba(self, X)-> ndarray:\n",
    "        preds = self.model.forward(X)\n",
    "        preds = preds.reshape(-1, 1)\n",
    "        \n",
    "        return preds\n",
    "    \n",
    "    def predict(self, X)-> ndarray:\n",
    "        preds = self.model.forward(X)\n",
    "        preds = preds.reshape(-1, 1)\n",
    "        \n",
    "        y_pred = np.zeros((preds.shape[0], 1))\n",
    "        \n",
    "        for i in range(preds.shape[0]):\n",
    "            y_pred[i, 0] = 1 if preds[i, 0]>0.5 else 0\n",
    "            \n",
    "        assert(y_pred.shape == preds.shape)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Regressor(\n",
    "                 activation=Sigmoid(),\n",
    "                 loss=SoftmaxCrossEntropy(),\n",
    "                 seed = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the data, train-test split etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "data = breast_cancer.data\n",
    "target = breast_cancer.target\n",
    "features = breast_cancer.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "s = StandardScaler()\n",
    "data = s.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_2d_np(a: ndarray, \n",
    "          type: str=\"col\") -> ndarray:\n",
    "    '''\n",
    "    Turns a 1D Tensor into 2D\n",
    "    '''\n",
    "\n",
    "    assert a.ndim == 1, \\\n",
    "    \"Input tensors must be 1 dimensional\"\n",
    "    \n",
    "    if type == \"col\":        \n",
    "        return a.reshape(-1, 1)\n",
    "    elif type == \"row\":\n",
    "        return a.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=1)\n",
    "\n",
    "# make target 2d array\n",
    "y_train, y_test = to_2d_np(y_train), to_2d_np(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "\n",
    "def permute_data(X, y):\n",
    "    perm = np.random.permutation(X.shape[0])\n",
    "    return X[perm], y[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 100 epochs is 7.635\n",
      "Loss increased after epoch 200, \n",
      "                            final loss was 7.635, using the model from epoch 100\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegressor(model, SGD(lr=0.05))\n",
    "\n",
    "log_reg.fit(X_train, y_train, X_test, y_test,\n",
    "       epochs = 1000,\n",
    "       eval_every = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy -> [      0.8070]\n"
     ]
    }
   ],
   "source": [
    "print('accuracy -> {}'.format(sum(predictions == y_test) / y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def evaluate_classifier(clf, X_test):\n",
    "    \n",
    "    total: int = len(clf.predict(X_test))\n",
    "    thresh: float = 0.5\n",
    "    true_positives: int = 0\n",
    "    true_negatives: int = 0\n",
    "    false_positives: int = 0\n",
    "    false_negatives: int = 0\n",
    "        \n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    for i in range(y_pred.shape[0]):\n",
    "        if y_test[i,0] == 1 and y_pred[i,0] == 1:\n",
    "            true_positives += 1\n",
    "        elif y_test[i,0] == 0 and y_pred[i,0] == 0:\n",
    "            true_negatives += 1\n",
    "        elif y_test[i,0] == 1 and y_pred[i,0] == 0:\n",
    "            false_negatives += 1\n",
    "        elif y_test[i,0] == 0 and y_pred[i,0] == 1:\n",
    "            false_positives += 1\n",
    "        \n",
    "    print(f'True Positives: {true_positives}')\n",
    "    print(f'True Negatives: {true_negatives}')\n",
    "    print(f'False Positives: {false_positives}')\n",
    "    print(f'False Negatives: {false_negatives}')\n",
    "    print(f'Accuracy: {(true_positives + true_negatives) / total}')\n",
    "    print(f'Error rate: {(false_positives + false_negatives) / total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 51\n",
      "True Negatives: 41\n",
      "False Positives: 1\n",
      "False Negatives: 21\n",
      "Accuracy: 0.8070175438596491\n",
      "Error rate: 0.19298245614035087\n"
     ]
    }
   ],
   "source": [
    "evaluate_classifier(log_reg, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[41  1]\n",
      " [21 51]]\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.66      0.98      0.79        42\n",
      "     class 1       0.98      0.71      0.82        72\n",
      "\n",
      "    accuracy                           0.81       114\n",
      "   macro avg       0.82      0.84      0.81       114\n",
      "weighted avg       0.86      0.81      0.81       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = y_test\n",
    "y_pred = log_reg.predict(X_test)\n",
    "target_names = ['class 0', 'class 1']\n",
    "\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print()\n",
    "print('Classification report:')\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying our Logistic Regressor on Digits Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "digits,target = datasets.load_digits(n_class=2,return_X_y=True)\n",
    "fig,ax = plt.subplots(nrows=1, ncols=10,figsize=(12,4),subplot_kw=dict(xticks=[], yticks=[]))\n",
    "\n",
    "# Plot some images of digits\n",
    "for i in np.arange(10):\n",
    "    ax[i].imshow(digits[i,:].reshape(8,8), cmap=plt.cm.gray)   \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scaling the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "s = StandardScaler()\n",
    "data = s.fit_transform(digits)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=1)\n",
    "\n",
    "# make target 2d array\n",
    "y_train, y_test = to_2d_np(y_train), to_2d_np(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Regressor(\n",
    "                 activation=Sigmoid(),\n",
    "                 loss=SoftmaxCrossEntropy(),\n",
    "                 seed = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 100 epochs is 10.074\n",
      "Loss increased after epoch 200, \n",
      "                            final loss was 10.074, using the model from epoch 100\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegressor(model, SGD(lr=0.05))\n",
    "\n",
    "log_reg.fit(X_train, y_train, X_test, y_test,\n",
    "       epochs = 1000,\n",
    "       eval_every = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = log_reg.predict(X_test)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy -> [      0.9444]\n"
     ]
    }
   ],
   "source": [
    "print('accuracy -> {}'.format(sum(predictions == y_test) / y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = log_reg.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[33  2]\n",
      " [ 2 35]]\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.94      0.94      0.94        35\n",
      "     class 1       0.95      0.95      0.95        37\n",
      "\n",
      "    accuracy                           0.94        72\n",
      "   macro avg       0.94      0.94      0.94        72\n",
      "weighted avg       0.94      0.94      0.94        72\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = y_test\n",
    "y_pred = log_reg.predict(X_test)\n",
    "target_names = ['class 0', 'class 1']\n",
    "\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print()\n",
    "print('Classification report:')\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test[1].reshape(8,8), cmap=plt.cm.binary);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[      1.0000]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.predict(X_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=1)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.fit_transform(X_test)\n",
    "\n",
    "# define bounds of the domain\n",
    "min1, max1 = X[:, 0].min()-1, X[:, 0].max()+1\n",
    "min2, max2 = X[:, 1].min()-1, X[:, 1].max()+1\n",
    "# define the x and y scale\n",
    "x1grid = np.arange(min1, max1, 0.1)\n",
    "x2grid = np.arange(min2, max2, 0.1)\n",
    "# create all of the lines and rows of the grid\n",
    "xx, yy = np.meshgrid(x1grid, x2grid)\n",
    "# flatten each grid to a vector\n",
    "r1, r2 = xx.flatten(), yy.flatten()\n",
    "r1, r2 = r1.reshape((len(r1), 1)), r2.reshape((len(r2), 1))\n",
    "# horizontal stack vectors to create x1,x2 input for the model\n",
    "grid = np.hstack((r1,r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[     -2.3922,      -5.2639],\n",
       "       [     -4.2958,      -4.0111],\n",
       "       [      3.7778,      -1.7054],\n",
       "       [     -3.2344,       1.2223],\n",
       "       [     -3.6278,       6.6492],\n",
       "       [      3.8790,      -1.0725],\n",
       "       [      5.8986,       0.7292],\n",
       "       [     -1.0305,       0.2751],\n",
       "       [      4.5702,      -0.2345],\n",
       "       [      2.3862,       0.9361],\n",
       "       [      2.9758,      -1.2140],\n",
       "       [      2.9650,      -0.1465],\n",
       "       [     -3.9428,      -3.4601],\n",
       "       [      4.4340,       0.2669],\n",
       "       [      0.6572,       0.8173],\n",
       "       [     -3.0755,       7.2484],\n",
       "       [      4.8518,      -0.8130],\n",
       "       [      3.2681,       1.0879],\n",
       "       [      6.9511,       0.0436],\n",
       "       [      3.8594,      -0.7410],\n",
       "       [      4.1620,       1.8425],\n",
       "       [      2.7633,      -0.1609],\n",
       "       [     -3.7482,      -3.8958],\n",
       "       [     -3.8642,      -4.6013],\n",
       "       [     -3.7439,       3.5503],\n",
       "       [      4.1812,       0.3043],\n",
       "       [      1.5737,       0.9086],\n",
       "       [     -2.1348,       6.5665],\n",
       "       [      5.2883,       0.2239],\n",
       "       [      4.7733,      -0.5472],\n",
       "       [     -2.3189,      -2.4039],\n",
       "       [     -4.0203,       0.9764],\n",
       "       [     -4.4744,       2.1480],\n",
       "       [     -4.2069,      -4.4091],\n",
       "       [      3.6427,      -0.0258],\n",
       "       [     -4.3166,       2.5670],\n",
       "       [      2.0285,      -0.3007],\n",
       "       [     -4.5841,      -0.8383],\n",
       "       [      6.2042,      -0.8478],\n",
       "       [      3.5215,      -0.2463],\n",
       "       [     -4.2631,       5.4641],\n",
       "       [     -5.5113,      -4.1013],\n",
       "       [     -4.0568,       3.3751],\n",
       "       [      3.2107,      -0.5497],\n",
       "       [      4.2123,      -1.4473],\n",
       "       [      3.9765,       0.2461],\n",
       "       [      4.6428,       0.2913],\n",
       "       [     -3.9244,      -2.8038],\n",
       "       [     -4.6703,      -4.1509],\n",
       "       [     -1.0576,      -2.9524],\n",
       "       [      3.0796,      -1.2314],\n",
       "       [     -3.5053,      -1.9598],\n",
       "       [      3.4743,       0.5302],\n",
       "       [     -4.1217,      -4.8195],\n",
       "       [     -3.1629,       5.2553],\n",
       "       [     -1.0085,       7.0741],\n",
       "       [     -5.4589,       0.5369],\n",
       "       [      3.5574,       0.1845],\n",
       "       [      6.7978,      -0.7842],\n",
       "       [      3.9132,      -0.3322],\n",
       "       [      2.8820,       0.3875],\n",
       "       [     -2.6588,      -4.4256],\n",
       "       [     -4.1931,      -3.0812],\n",
       "       [      4.6182,      -0.6063],\n",
       "       [      4.5070,      -0.4302],\n",
       "       [     -4.5455,      -3.3576],\n",
       "       [      3.5258,      -0.2514],\n",
       "       [     -0.4795,       7.1644],\n",
       "       [      3.3639,      -0.4706],\n",
       "       [     -3.5236,       2.6326],\n",
       "       [     -2.3392,       7.8471],\n",
       "       [      3.5622,       1.3206],\n",
       "       [      3.0229,       0.2713],\n",
       "       [     -2.9374,      -3.0014],\n",
       "       [      3.5062,      -1.5382],\n",
       "       [     -4.7688,       2.1884],\n",
       "       [     -5.5737,      -2.3606],\n",
       "       [      4.3674,      -0.6716],\n",
       "       [     -2.9570,       8.0519],\n",
       "       [     -1.1250,       7.0907],\n",
       "       [      3.1389,      -0.3282],\n",
       "       [     -4.3215,      -2.8376],\n",
       "       [      4.2195,       1.1562],\n",
       "       [     -5.1329,      -1.2190],\n",
       "       [     -1.9339,       6.7605],\n",
       "       [     -3.6056,      -0.9343],\n",
       "       [     -2.1938,      -2.4053],\n",
       "       [     -3.8624,       3.5496],\n",
       "       [     -2.2731,      -5.4425],\n",
       "       [     -4.0600,       4.2391],\n",
       "       [      2.2269,       0.0187],\n",
       "       [      6.1505,      -1.2271],\n",
       "       [     -2.1872,       5.7027],\n",
       "       [      3.5158,       0.0718],\n",
       "       [     -2.4879,       7.0784],\n",
       "       [     -5.0051,       2.3121],\n",
       "       [      4.4287,      -1.0132],\n",
       "       [      5.0258,       0.3247],\n",
       "       [      3.5012,      -0.8589],\n",
       "       [     -3.5168,       5.3573],\n",
       "       [     -3.3169,      -2.9366],\n",
       "       [      3.5618,       0.0323],\n",
       "       [      4.4249,      -0.0297],\n",
       "       [     -0.9836,       7.0073],\n",
       "       [     -2.4892,       4.7731],\n",
       "       [      4.6850,      -0.7291],\n",
       "       [     -5.0843,      -2.9477],\n",
       "       [     -5.1742,      -1.4844],\n",
       "       [     -1.9398,       7.7134],\n",
       "       [     -4.3738,      -2.6409],\n",
       "       [      4.1033,      -0.4973],\n",
       "       [      3.8399,      -0.7535],\n",
       "       [     -4.3591,      -0.9845],\n",
       "       [     -1.8505,      -0.7058],\n",
       "       [      3.0435,      -0.7985],\n",
       "       [     -3.2977,       5.1906],\n",
       "       [      4.8796,       0.2889],\n",
       "       [     -3.8590,       5.1869],\n",
       "       [     -3.9967,      -3.6196],\n",
       "       [     -4.6486,      -2.4677],\n",
       "       [      2.5022,       0.3146],\n",
       "       [     -3.7192,       4.4681],\n",
       "       [     -4.7084,       0.0831],\n",
       "       [      3.2129,      -0.2844],\n",
       "       [     -1.3939,      -0.7903],\n",
       "       [      2.9256,      -0.7995],\n",
       "       [     -4.3072,      -1.8740],\n",
       "       [     -2.7388,      -0.5056],\n",
       "       [      4.6109,       0.0897],\n",
       "       [     -3.9064,       5.1761],\n",
       "       [     -0.7876,       7.5056],\n",
       "       [      3.3394,      -0.1210],\n",
       "       [     -3.2657,      -5.4849],\n",
       "       [      4.6762,      -0.2644],\n",
       "       [      6.5265,      -0.4977],\n",
       "       [      3.2914,       2.0863],\n",
       "       [      6.2928,      -1.0432],\n",
       "       [      0.7067,       2.0260],\n",
       "       [     -2.1326,      -2.9258],\n",
       "       [     -2.8695,       4.3608],\n",
       "       [     -3.5020,       2.9639],\n",
       "       [      3.1248,      -0.3897],\n",
       "       [     -5.1266,       0.0801],\n",
       "       [      3.1940,      -0.7263],\n",
       "       [     -2.2169,       4.6873],\n",
       "       [      5.5588,       0.4824],\n",
       "       [      3.7131,       0.2162],\n",
       "       [      2.6931,       1.1055],\n",
       "       [      3.3514,       0.7338],\n",
       "       [      2.8980,       0.7658],\n",
       "       [     -5.2720,      -2.6650],\n",
       "       [     -3.1075,       6.2871],\n",
       "       [     -4.1147,       2.2909],\n",
       "       [     -3.6735,      -4.0237],\n",
       "       [      3.8718,      -0.0500],\n",
       "       [     -3.7970,       0.0362],\n",
       "       [      2.8004,       1.2805],\n",
       "       [     -3.1896,      -0.9376],\n",
       "       [      2.7397,       0.1485],\n",
       "       [      4.0611,      -1.0508],\n",
       "       [     -4.6055,      -2.4888],\n",
       "       [      3.5041,      -0.0306],\n",
       "       [     -3.8689,      -2.1286],\n",
       "       [      5.1286,      -0.7706],\n",
       "       [     -3.3303,      -5.2483],\n",
       "       [     -4.8183,      -2.5638],\n",
       "       [     -2.8798,      -1.0100],\n",
       "       [      5.3924,      -1.4384],\n",
       "       [      3.5027,      -0.2183],\n",
       "       [     -3.3880,      -5.4735],\n",
       "       [     -3.9917,      -0.8846],\n",
       "       [     -4.6892,       4.0264],\n",
       "       [      3.8215,       0.6246],\n",
       "       [      3.5136,      -0.3056],\n",
       "       [     -3.3563,      -1.5085],\n",
       "       [      5.3647,      -0.8348],\n",
       "       [     -4.2534,       1.7352],\n",
       "       [      5.8876,       0.0469],\n",
       "       [      3.6345,       0.0045],\n",
       "       [      3.3758,      -0.7059],\n",
       "       [      3.7978,       0.0139],\n",
       "       [     -3.9473,      -3.6712],\n",
       "       [      5.5035,       0.1077],\n",
       "       [     -4.5350,      -3.1904],\n",
       "       [      3.0303,       0.2743],\n",
       "       [     -5.2301,       1.5568],\n",
       "       [      5.5695,      -0.7464],\n",
       "       [     -3.2085,       0.9830],\n",
       "       [      3.3216,      -1.2512],\n",
       "       [     -4.6404,       1.6054],\n",
       "       [     -5.3430,      -2.7003],\n",
       "       [     -3.9408,       0.8928],\n",
       "       [     -3.9078,       1.5765],\n",
       "       [      4.8412,      -0.6369],\n",
       "       [      5.2306,      -0.9160],\n",
       "       [     -5.2425,      -2.6241],\n",
       "       [     -4.2705,       3.9146],\n",
       "       [      4.2867,      -1.1685],\n",
       "       [      3.0899,      -0.9068],\n",
       "       [     -4.6015,      -2.3557],\n",
       "       [      3.5188,      -0.9799],\n",
       "       [      6.6029,      -1.3888],\n",
       "       [     -4.0198,      -1.4571],\n",
       "       [     -1.9636,      -1.5061],\n",
       "       [     -5.6264,      -1.9496],\n",
       "       [      2.7360,       1.1297],\n",
       "       [      3.5236,       0.2357],\n",
       "       [     -1.3221,       6.5441],\n",
       "       [     -2.3115,      -3.7631],\n",
       "       [      2.3814,       0.7368],\n",
       "       [      4.0762,      -0.3435],\n",
       "       [     -3.1143,      -1.3471],\n",
       "       [     -5.3540,      -1.7184],\n",
       "       [     -3.2421,      -4.0785],\n",
       "       [     -4.8681,      -1.8179],\n",
       "       [      3.5145,      -0.0976],\n",
       "       [     -2.3714,      -2.7040],\n",
       "       [     -3.9462,       2.1162],\n",
       "       [     -4.2070,      -1.6679],\n",
       "       [     -5.0745,       0.1335],\n",
       "       [      6.7135,      -1.5514],\n",
       "       [     -5.7323,      -2.4655],\n",
       "       [     -2.9634,       6.0891],\n",
       "       [      2.3651,      -0.1422],\n",
       "       [      3.8750,      -0.0549],\n",
       "       [     -2.9306,       4.5652],\n",
       "       [      2.5569,       0.4752],\n",
       "       [     -4.5795,      -3.5180],\n",
       "       [     -3.4338,      -1.4985],\n",
       "       [      4.2273,      -0.4908],\n",
       "       [     -5.4379,      -2.8593],\n",
       "       [      2.9852,       1.0256],\n",
       "       [      3.8840,       0.8147],\n",
       "       [      2.8568,       0.0089],\n",
       "       [      2.3826,      -0.1061],\n",
       "       [     -4.9844,      -1.1975],\n",
       "       [      2.7652,      -0.0472],\n",
       "       [      4.1017,      -1.5224],\n",
       "       [      4.0412,      -0.5349],\n",
       "       [      3.0308,      -0.0081],\n",
       "       [     -0.9081,       6.2857],\n",
       "       [     -3.9615,      -4.7528],\n",
       "       [      4.2552,      -0.7795],\n",
       "       [      3.2864,      -0.6585],\n",
       "       [     -1.6776,      -4.2774],\n",
       "       [      2.6524,      -0.1715],\n",
       "       [      3.5828,      -0.1060],\n",
       "       [     -4.4998,       4.9457],\n",
       "       [     -5.6118,      -1.5650],\n",
       "       [      5.7195,      -0.9071],\n",
       "       [      3.7618,      -0.7960],\n",
       "       [      2.8575,      -1.3128],\n",
       "       [      2.6095,       1.4390],\n",
       "       [      6.4211,      -0.2782],\n",
       "       [     -3.5949,       0.8319],\n",
       "       [      3.2451,      -1.8072],\n",
       "       [      1.4288,       0.2229],\n",
       "       [      2.5825,      -1.5081],\n",
       "       [      3.2640,      -0.6371],\n",
       "       [     -5.1290,      -0.1892],\n",
       "       [      2.3913,      -0.6549],\n",
       "       [     -4.1459,      -2.8380],\n",
       "       [     -5.7710,      -2.1737],\n",
       "       [     -0.2062,      -0.2966],\n",
       "       [      2.9316,      -0.1498],\n",
       "       [      2.6038,      -0.6554],\n",
       "       [     -3.4307,       4.8113],\n",
       "       [      6.9806,       0.0898],\n",
       "       [      1.4479,      -0.4683],\n",
       "       [      4.4877,       0.9558],\n",
       "       [     -2.8879,       7.0261],\n",
       "       [      4.3499,      -0.5328],\n",
       "       [     -4.1374,      -2.8239],\n",
       "       [      1.9382,       0.0486],\n",
       "       [     -5.2401,      -0.9061],\n",
       "       [     -4.7818,      -2.5237],\n",
       "       [     -4.3447,      -3.3486],\n",
       "       [      2.6772,      -0.4397],\n",
       "       [     -4.8580,       1.8930],\n",
       "       [      4.5507,       0.8251],\n",
       "       [     -3.2105,      -0.5341],\n",
       "       [     -2.8419,      -1.5490],\n",
       "       [     -3.2805,      -2.9153],\n",
       "       [     -3.4078,      -4.4833],\n",
       "       [     -1.2300,      -4.9353],\n",
       "       [     -4.9066,      -1.1023],\n",
       "       [      3.4876,      -0.6674],\n",
       "       [      1.1144,       1.0606]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pca"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
